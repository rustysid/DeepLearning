{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon, nd, autograd\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import csv\n",
    "import pickle\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = list()\n",
    "X_test = list()\n",
    "X_validate = list()\n",
    "\n",
    "csvtrain = csv.reader(open(\"/Users/root02/Downloads/emnist-balanced-train.csv\"))\n",
    "csvtest = csv.reader(open(\"/Users/root02/Downloads/emnist-balanced-test.csv\"))\n",
    "\n",
    "for row_ in csvtrain:\n",
    "    X_train.append (((np.array(row_[1:]).reshape((784, 1)).reshape((28, 28))), row_[0]))\n",
    "\n",
    "for row_1 in csvtest:\n",
    "    X_test.append ((np.array(row_1[1:]).reshape((784, 1)).reshape((28, 28)), row_1[0]))\n",
    "    \n",
    "random.shuffle(X_train)\n",
    "X_validate = X_train[:10000]\n",
    "X_train = X_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define EMNIST Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMNIST(gluon.data.Dataset):\n",
    "    \n",
    "    def __init__ (self, list_data, transform=None):\n",
    "        self._data = list_data\n",
    "        self._transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self._transform(self._data[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create transform function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_tuple(tuple_data):\n",
    "    return nd.array(tuple_data[0].reshape(-1, 28, 28).astype(np.float32)/255), int(tuple_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = gluon.data.DataLoader(EMNIST(X_train, transform=transform_tuple), batch_size=64, shuffle=True)\n",
    "validation_data = gluon.data.DataLoader(EMNIST(X_validate, transform=transform_tuple), batch_size=64, shuffle=False)\n",
    "test_data = gluon.data.DataLoader(EMNIST(X_test, transform=transform_tuple), batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = gluon.nn.Sequential()\n",
    "\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Conv2D(channels=64, kernel_size=(5, 5), activation=\"relu\"))\n",
    "    net.add(gluon.nn.Conv2D(channels=64, kernel_size=(3, 3), activation=\"relu\"))\n",
    "    net.add(gluon.nn.MaxPool2D())\n",
    "    net.add(gluon.nn.Dense(64, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(64, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(47))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "net.collect_params().initialize(mx.init.Xavier(), ctx = context)\n",
    "trainer = gluon.Trainer(net.collect_params(), \"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_acc(net, dataset):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for idx, (data, label) in enumerate(dataset):\n",
    "        output = net(data)\n",
    "        preds = nd.argmax(output, axis = 1)\n",
    "        acc.update(preds=preds, labels=label)\n",
    "    return acc.get()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(net, dataset, num_epochs = 1):\n",
    "    for i in range(num_epochs):\n",
    "        for idx, (data, label) in enumerate(dataset):\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss = cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(data.shape[0])\n",
    "            sys.stdout.write(\"\\rPercentage completed: \"+str((idx+1)/len(dataset)*100)[:10])\n",
    "            sys.stdout.flush()\n",
    "        print (\"\\nEpoch %s / Training Accuracy: %s, Test Accuracy: %s\" % (str(i+1), str(eval_acc(net, training_data)), str(eval_acc(net, validation_data))))\n",
    "    print (\"Training completed! / Training Accuracy: %s, Test Accuracy: %s\" % (str(eval_acc(net, training_data)), str(eval_acc(net, validation_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed! / Training Accuracy: 0.849873540856, Test Accuracy: 0.8396\n"
     ]
    }
   ],
   "source": [
    "training_loop(net, training_data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 measures: avoidable bias and variance.\n",
    "\n",
    "1) Bayes Error: Bayes Error is the minimum possible error that is achievable. In this use-case, we can assume ~0% error.\n",
    "\n",
    "2) Avoidable Bias: Bayes Error - Training Error\n",
    "\n",
    "3) Variance: Training Error - Test Error\n",
    "\n",
    "Bayes Error = 0\n",
    "\n",
    "Training Error = 15%\n",
    "\n",
    "Test Error = 16%\n",
    "\n",
    "Avoidable Bias = 15%\n",
    "Variance = 1%\n",
    "\n",
    "2-3 techniques:\n",
    "\n",
    "1) Train for more epochs\n",
    "\n",
    "2) Train a more complex network\n",
    "\n",
    "3) Use batch normalization to speed up convergence (sometimes this helps accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
